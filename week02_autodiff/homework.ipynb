{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOb-hGR2uh7t"
   },
   "source": [
    "# Homework part I\n",
    "\n",
    "The first problem set contains basic tasks in PyTorch.\n",
    "\n",
    "__Note:__ Instead of doing this part of homework, you can prove your skills otherwise:\n",
    "* A commit to PyTorch or PyTorch-based repos will do;\n",
    "* Fully implemented seminar assignment in tensorflow or theano will do;\n",
    "* Your own project in PyTorch that is developed to a state in which a normal human can understand and appreciate what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCFZeFlGuh7v",
    "outputId": "d8c05d93-221e-4103-e0e7-e9367f4ae8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuMIhPfYuh71"
   },
   "source": [
    "### Task I - tensormancy\n",
    "\n",
    "![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n",
    "\n",
    "When dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword. \n",
    "\n",
    "\n",
    "__1.1 The Cannabola__\n",
    "[(_disclaimer_)](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)\n",
    "\n",
    "Let's write another function, this time in polar coordinates:\n",
    "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (8 \\cdot \\theta) ) \\cdot (1 + 0.1 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.9 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (1 + sin(\\theta))$$\n",
    "\n",
    "\n",
    "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
    "\n",
    "Use torch tensors only: no lists, loops, numpy arrays, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "URx7y3hyuh72",
    "outputId": "89759738-0c47-4d5b-ef37-075fe9c26bd1"
   },
   "outputs": [],
   "source": [
    "theta = torch.linspace(-np.pi, np.pi, steps=1000)\n",
    "\n",
    "# compute rho(theta) as per formula above\n",
    "rho = (1 + 0.9 * torch.cos(8 * theta)) * (1 + 0.1 * torch.cos(24 * theta)) * (0.9 + 0.05 * torch.cos(200 * theta)) * (1 + torch.sin(theta))### YOUR CODE\n",
    "\n",
    "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
    "x = rho * torch.cos(theta)  ### YOUR CODE\n",
    "y = rho * torch.sin(theta)  ### YOUR CODE\n",
    "\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.fill(x.numpy(), y.numpy(), color='green')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eHnJPqyuh76"
   },
   "source": [
    "### Task II: The Game of Life\n",
    "\n",
    "Now it's time for you to make something more challenging. We'll implement Conway's [Game of Life](http://web.stanford.edu/~cdebs/GameOfLife/) in _pure PyTorch_.\n",
    "\n",
    "While this is still a toy task, implementing game of life this way has one cool benefit: __you'll be able to run it on GPU!__ Indeed, what could be a better use of your GPU than simulating Game of Life on 1M/1M grids?\n",
    "\n",
    "![img](https://cdn.tutsplus.com/gamedev/authors/legacy/Stephane%20Beniak/2012/09/11/Preview_Image.png)\n",
    "If you've skipped the URL above out of sloth, here's the Game of Life:\n",
    "* You have a 2D grid of cells, where each cell is \"alive\"(1) or \"dead\"(0)\n",
    "* Any living cell that has 2 or 3 neighbors survives, else it dies [0,1 or 4+ neighbors]\n",
    "* Any cell with exactly 3 neighbors becomes alive (if it was dead)\n",
    "\n",
    "For this task, you are given a reference NumPy implementation that you must convert to PyTorch.\n",
    "_[NumPy code inspired by: https://github.com/rougier/numpy-100]_\n",
    "\n",
    "\n",
    "__Note:__ You can find convolution in `torch.nn.functional.conv2d(Z,filters)`. Note that it has a different input format.\n",
    "\n",
    "__Note 2:__ From the mathematical standpoint, PyTorch convolution is actually cross-correlation. Those two are very similar operations. More info: [video tutorial](https://www.youtube.com/watch?v=C3EEy8adxvc), [scipy functions review](http://programmerz.ru/questions/26903/2d-convolution-in-python-similar-to-matlabs-conv2-question), [stack overflow source](https://stackoverflow.com/questions/31139977/comparing-matlabs-conv2-with-scipys-convolve2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_8ydkevuh78"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import correlate2d\n",
    "\n",
    "def np_update(Z):\n",
    "    # Count neighbours with convolution\n",
    "    filters = np.array([[1, 1, 1],\n",
    "                        [1, 0, 1],\n",
    "                        [1, 1, 1]])\n",
    "    \n",
    "    print(Z.shape)\n",
    "\n",
    "    N = correlate2d(Z, filters, mode='same')\n",
    "\n",
    "    # Apply rules\n",
    "    birth = (N == 3) & (Z == 0)\n",
    "    survive = ((N == 2) | (N == 3)) & (Z == 1)\n",
    "\n",
    "    Z[:] = birth | survive\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EX2Vii8uh7_"
   },
   "outputs": [],
   "source": [
    "def torch_update(Z):\n",
    "    \"\"\"\n",
    "    Implement an update function that does to Z exactly the same as np_update.\n",
    "    :param Z: torch.FloatTensor of shape [height,width] containing 0s(dead) an 1s(alive)\n",
    "    :returns: torch.FloatTensor Z after updates.\n",
    "    \n",
    "    You can opt to create new tensor or change Z inplace.\n",
    "    \"\"\"\n",
    "    \n",
    "    #<Your code here!>\n",
    "    filter = torch.tensor([[[[1, 1, 1],\n",
    "                          [1, 0, 1],\n",
    "                          [1, 1, 1]]]], dtype=torch.float32)\n",
    "    Z = Z.unsqueeze(0)\n",
    "    Z = Z.unsqueeze(0)\n",
    "    N = torch.conv2d(Z, filter, padding=1)\n",
    "\n",
    "    birth = (N == 3) & (Z == 0)\n",
    "    survive = ((N == 2) | (N == 3)) & (Z == 1)\n",
    "\n",
    "    Z[:] = birth | survive\n",
    "    \n",
    "    Z = Z.squeeze()\n",
    "    \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rX2wAml2uh8C",
    "outputId": "02dbcdc6-17e4-4d40-e9fe-d729ffd12b36"
   },
   "outputs": [],
   "source": [
    "# initial frame\n",
    "Z_numpy = np.random.choice([0, 1], p=(0.5, 0.5), size=(100, 100))\n",
    "Z = torch.from_numpy(Z_numpy).type(torch.FloatTensor)\n",
    "\n",
    "# your debug polygon :)\n",
    "Z_new = torch_update(Z.clone())\n",
    "\n",
    "# tests\n",
    "Z_reference = np_update(Z_numpy.copy())\n",
    "assert np.all(Z_new.numpy() == Z_reference), \\\n",
    "    \"your PyTorch implementation doesn't match np_update. Look into Z and np_update(ZZ) to investigate.\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c_KneQpuh8G"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "\n",
    "# initialize game field\n",
    "Z = np.random.choice([0, 1], size=(100, 100))\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    # update\n",
    "    Z = torch_update(Z)\n",
    "\n",
    "    # re-draw image\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(), cmap='gray')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXlR0iJjuh8L"
   },
   "outputs": [],
   "source": [
    "# Some fun setups for your amusement\n",
    "\n",
    "# parallel stripes\n",
    "Z = np.arange(100) % 2 + np.zeros([100, 100])\n",
    "# with a small imperfection\n",
    "Z[48:52, 50] = 1\n",
    "\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    Z = torch_update(Z)\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(), cmap='gray')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HpYcyniuh8P"
   },
   "source": [
    "More fun with Game of Life: [video](https://www.youtube.com/watch?v=C2vgICfQawE) and/or [Jupyter Notebook](https://nbviewer.jupyter.org/url/norvig.com/ipython/Life.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMvE8UoHuh8Q"
   },
   "source": [
    "### Task III: Going deeper\n",
    "<img src=\"http://download.gamezone.com/uploads/image/data/1190338/article_post_width_a88.jpg\" width=360>\n",
    "\n",
    "Your third trial is to build your first neural network [almost] from scratch and pure PyTorch.\n",
    "\n",
    "This time you will solve yet another digit recognition problem, but at a greater scale\n",
    "\n",
    "* 10 different letters\n",
    "* 20k samples\n",
    "\n",
    "We want you to build a network that reaches at least 80% accuracy and has at least 2 linear layers in it. Naturally, it should be nonlinear to beat logistic regression.\n",
    "\n",
    "\n",
    "With 10 classes you will need to use __Softmax__ at the top instead of sigmoid and train using __categorical crossentropy__  (see [here](http://wiki.fast.ai/index.php/Log_Loss)).  Write your own loss or use `torch.nn.functional.nll_loss`. Just make sure you understand what it accepts as input.\n",
    "\n",
    "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) neural network should already give you an edge over logistic regression.\n",
    "\n",
    "\n",
    "__[bonus kudos]__\n",
    "If you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! It should be possible to reach 90% without convnets.\n",
    "\n",
    "__SPOILERS!__\n",
    "At the end of the notebook you will find a few tips and frequent errors.\n",
    "If you feel confident enough, just start coding right away and get there ~~if~~ once you need to untangle yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV5HZ-jXoT-R"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall21/week02_autodiff/notmnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1NcwbJLuh8R",
    "outputId": "29382e11-0ebc-49e2-e14d-77b4315efef0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from notmnist import load_notmnist\n",
    "X_train, y_train, X_test, y_test = load_notmnist(letters='ABCDEFGHIJ')\n",
    "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26anEwnwuh8V",
    "outputId": "beffb493-6c1e-4098-e3e2-f5f3801b9b09"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=[12, 4])\n",
    "for i in range(20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape([28, 28]))\n",
    "    plt.title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "class NotMNISTDataset(Dataset):\n",
    "        \n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "train_not_mnist_dataset = NotMNISTDataset(images=X_train, labels=y_train)\n",
    "train_not_mnist_dataloader = DataLoader(\n",
    "    train_not_mnist_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "test_not_mnist_dataset = NotMNISTDataset(images=X_test, labels=y_test)\n",
    "test_not_mnist_dataloader = DataLoader(\n",
    "    test_not_mnist_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 64),  \n",
    "    nn.ReLU(),       \n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1))\n",
    "                      \n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "accs = []\n",
    "accs_train = []\n",
    "train_losses = []\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    \n",
    "    for batch in train_not_mnist_dataloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        predicts =  model.forward(images)  # YOUR CODE\n",
    "\n",
    "        loss_value =  criterion(predicts, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss_value.item()\n",
    "        accuracy = (predicts.argmax(dim=1) == labels).sum() / BATCH_SIZE\n",
    "        avg_acc += accuracy\n",
    "        \n",
    "    avg_acc /= len(train_not_mnist_dataloader)\n",
    "    accs_train.append(avg_acc)\n",
    "\n",
    "    train_losses.append(avg_loss/len(train_not_mnist_dataloader))\n",
    "\n",
    "    avg_acc = 0\n",
    "        \n",
    "    for batch in test_not_mnist_dataloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predicts = model(images)\n",
    "\n",
    "        accuracy = (predicts.argmax(dim=1) == labels).sum() / BATCH_SIZE\n",
    "        avg_acc += accuracy\n",
    "        \n",
    "    avg_acc /= len(test_not_mnist_dataloader)\n",
    "    accs.append(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accs = [acc.cpu() for acc in accs]\n",
    "accs_train = [acc.cpu() for acc in accs_train] \n",
    "plt.grid()\n",
    "plt.plot(np.arange(0, n_epochs), accs, label='test_data')\n",
    "plt.plot(np.arange(0, n_epochs), accs_train, label = 'train_data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_test =  model.forward(torch.tensor(X_test, device=device)).argmax(dim=1).cpu().data.numpy()\n",
    "\n",
    "accuracy = np.mean(predicted_y_test == y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3noJGwUSoT-S"
   },
   "source": [
    "# SPOILERS!\n",
    "\n",
    "Recommended pipeline:\n",
    "\n",
    "* Adapt logistic regression from seminar assignment to classify one letter against others (e.g. A vs the rest)\n",
    "* Generalize it to multiclass logistic regression.\n",
    "  - Either try to remember lecture 0 or google it.\n",
    "  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
    "  - Softmax (exp over sum of exps) can be implemented manually or as `nn.Softmax` (layer) or `F.softmax` (function)\n",
    "  - Probably better to use STOCHASTIC gradient descent (minibatch) for greater speed\n",
    "    - You can also try momentum/rmsprop/adawhatever\n",
    "    - in which case the dataset should probably be shuffled (or use random subsamples on each iteration)\n",
    "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n",
    "  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (e.g. sigmoid) instead of softmax\n",
    "  - You need to train both layers, not just the output layer :)\n",
    "  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve.\n",
    "  - In ideal case this totals to 2 `torch.matmul`'s, 1 softmax and 1 ReLU/sigmoid\n",
    "  - __Make sure this neural network works better than logistic regression!__\n",
    "\n",
    "* Now's the time to try improving the network. Consider layers (size, neuron count), nonlinearities, optimization methods, initialization â€” whatever you want, but please avoid convolutions for now.\n",
    "\n",
    "* If anything seems wrong, try going through one step of training and printing everything you compute.\n",
    "* If you see NaNs midway through optimization, you can estimate $\\log P(y \\mid x)$ as `F.log_softmax(layer_before_softmax)`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
